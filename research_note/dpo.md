# DPO

https://arxiv.org/abs/2305.18290

Direct Preference Optimization: Your Language Model is Secretly a Reward Model

This is a counterpart of RLHF. Without using reward model in RL, it can be used to tune a language model.
