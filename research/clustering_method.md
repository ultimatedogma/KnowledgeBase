Great, I’ll look into the specific clustering algorithms commonly used in genetics and single-cell research, especially in the context of handling large datasets. I’ll include examples of how each algorithm is applied, particularly in identifying cell types or patterns in gene expression.

I’ll get back to you with a detailed breakdown shortly.

# Clustering Algorithms in Genetics and Single-Cell Research

## Introduction  
Clustering is a fundamental technique in genetics and single-cell biology for discovering natural groupings in high-dimensional data. In single-cell transcriptomics (e.g. scRNA-seq), clustering helps identify cell types or states by grouping cells with similar gene expression profiles. In genetic studies (e.g. population genomics or GWAS), clustering can reveal subpopulations (ancestry groups) or patterns in genotype or gene expression data. These datasets are often **large** (thousands to millions of cells or individuals) and **high-dimensional** (thousands of genes or variants), so scalable clustering algorithms are critical. Common clustering methods can be categorized into four main families: **partitioning methods** (e.g. *k*-means), **hierarchical methods**, **graph-based community detection** (e.g. Louvain/Leiden), and **density-based methods** (e.g. DBSCAN) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=performed,to%20evaluate%20and%20compare%20those)). Each has advantages and limitations for genetic or single-cell data, as summarized below.

 ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/)) *Overview of a single-cell RNA-seq clustering workflow, showing major clustering method categories and example tools ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=performed,to%20evaluate%20and%20compare%20those)). Methods are grouped into k-means-based, hierarchical, graph-based (community detection), and density-based approaches. Upstream steps (quality control, normalization, dimensionality reduction) are typically performed before clustering in single-cell analyses.*

Before clustering, it’s common to perform dimensionality reduction (e.g. PCA) to denoise and project high-dimensional data into a more tractable space ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=workflow%20of%20single,and%20compare%20those%20clustering%20methods)). This is especially true in single-cell transcriptomics, where tens of thousands of gene features are often reduced to a few dozen principal components or latent features. Below, we discuss each clustering algorithm, how it works, why it’s suitable for genetic or single-cell applications, examples of its use, and how it scales to large high-dimensional data.

## K-Means Clustering  
**How it works:** *K*-means is a classic partitioning algorithm that iteratively assigns points to the nearest of *K* cluster centroids and then updates the centroids to minimize within-cluster variance. It seeks to minimize the sum of squared distances from each point to its cluster center ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=k,Lloyd%201982)). The user must choose *K* (the number of clusters) in advance. The algorithm is relatively fast, with time complexity roughly linear in the number of points (and can be accelerated with optimizations), making it feasible for large datasets ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=k,Lloyd%201982)). 

**Why it’s used in genetics/single-cell:** K-means is simple and was one of the earliest clustering methods applied to gene expression data. It works best when clusters are roughly spherical in the feature space, which often holds after PCA reduction of gene expression. K-means scales to large sample sizes, so it can handle thousands of cells or individuals. For example, in population genetics, investigators have used *K*-means on genotype-derived principal components to cluster individuals into ancestral groups ([PopMLvis: a tool for analysis and visualization of population structure using genotype data from genome-wide association studies | BMC Bioinformatics | Full Text](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-024-05908-1#:~:text=PopMLvis%20pipeline%2Fworkflow%3A%20,Download%20graphical%20plots%20and%20datasheets)). In single-cell RNA-seq, k-means (or its variants) has been used to partition cells by transcriptomic profile – sometimes as a standalone method or as part of hybrid approaches. For instance, the **RaceID** algorithm (Grün et al. 2015) applied k-means clustering to single-cell expression data to uncover rare intestinal cell types ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=2016%20,76)). Similarly, the **SC3** tool (Kiselev et al. 2017) ran multiple k-means clusterings on a dataset and combined them in a consensus, improving stability of cell-type identification ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=To%20overcome%20the%20above%20drawbacks%2C,suitable%20cluster%20number%2C%20which%20is)).

**Use cases and tools:** Early microarray studies often clustered genes or samples with k-means to find co-expression modules. In single-cell analysis, k-means is sometimes used for “over-clustering” (choosing a relatively large *K* to get many small clusters) which can then be merged or analyzed hierarchically ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=scRNA,seq%20and%20bulk%20data)). Tools like **SAIC** (Yang et al. 2017) use iterative k-means to refine gene signatures for clustering ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Several%20clustering%20tools%20based%20on,76)), and **pcaReduce** (Žurauskiene & Yau 2016) starts with k-means initial clusters then merges them hierarchically ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Several%20clustering%20tools%20based%20on,76)). In population structure analysis, k-means and related algorithms (e.g. fuzzy c-means) have been used to cluster individuals based on genome-wide SNP data after PCA, grouping individuals by genetic ancestry ([PopMLvis: a tool for analysis and visualization of population structure using genotype data from genome-wide association studies | BMC Bioinformatics | Full Text](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-024-05908-1#:~:text=PopMLvis%20pipeline%2Fworkflow%3A%20,Download%20graphical%20plots%20and%20datasheets)).

**Scalability:** K-means is efficient for large *N* (number of data points) – each iteration is O(N) given a fixed *K*, and it typically converges in a reasonable number of iterations ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=k,Lloyd%201982)). This linear scaling means it can cluster tens of thousands of cells or individuals. It also handles high-dimensional data if distance computations are feasible (often Euclidean distance on top principal components). However, in extremely high dimensions, distance metrics can become less meaningful (curse of dimensionality), so dimension reduction is advised. K-means requires enough memory to store the data and centroid distances but is not memory-intensive beyond that. It can struggle if *K* is large or if data contains many outliers.

- *Strengths:* Simple and fast; scales linearly with dataset size ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=k,Lloyd%201982)). Well-suited for well-separated, compact clusters; works efficiently on PCA-reduced gene expression data.  
- *Limitations:* Requires the user to specify the number of clusters *K* in advance. Assumes clusters are roughly globular/convex, so it may perform poorly if true clusters have irregular shapes or very different sizes. It is sensitive to outliers and can miss **rare cell populations** – a small distinct group might be assigned to the nearest larger cluster centroid instead of forming its own cluster ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=However%2C%20k,detecting%20of%20rare%20cell%20types)). Results can also vary with random initial centroid seeds (greedy algorithm may find a local optimum).  
- *Examples:* Clustering single-cell transcriptomes into cell types (if an expected number of cell types is known or guessed). RaceID used k-means to detect rare cell types in intestine ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=2016%20,76)). SC3 used an ensemble of k-means runs to improve robustness ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=To%20overcome%20the%20above%20drawbacks%2C,suitable%20cluster%20number%2C%20which%20is)). In GWAS, k-means on genotype principal components has been used to define population clusters for stratification control ([PopMLvis: a tool for analysis and visualization of population structure using genotype data from genome-wide association studies | BMC Bioinformatics | Full Text](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-024-05908-1#:~:text=PopMLvis%20pipeline%2Fworkflow%3A%20,Download%20graphical%20plots%20and%20datasheets)).  
- *Software:* K-means is available in practically all analysis platforms (e.g. **R**’s `kmeans()`, **Python**’s scikit-learn). However, popular single-cell packages like Seurat and Scanpy do **not** use plain k-means by default for clustering; instead, they favor graph-based methods (discussed below). K-means might still be used internally or optionally (for example, SC3 and RaceID in R as noted). Users can also apply k-means to low-dimensional embeddings (PCA, t-SNE, UMAP) if they prefer a simple clustering approach.

## Hierarchical Clustering  
**How it works:** Hierarchical clustering builds a tree (dendrogram) of the data points by either progressively merging smaller clusters (agglomerative) or splitting larger clusters (divisive). Agglomerative hierarchical clustering starts with each point as its own cluster and iteratively merges the closest pair of clusters until one cluster remains ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Hierarchical%20clustering%20is%20another%20widely,seq%20data.%20Hence%2C%20many)). This requires a definition of distance between clusters (linkage criteria, e.g. single linkage, complete linkage, average linkage). Divisive methods do the reverse: start with all points in one cluster and recursively split clusters. The result is a hierarchical tree structure where cutting the tree at a certain level yields a chosen number of clusters. A key feature is that one does **not** need to pre-specify the number of clusters – one can decide how to cut the dendrogram post hoc based on desired granularity. Hierarchical clustering also makes no explicit assumption about cluster shape (it relies on the distance metric). 

**Why it’s used in genetics/single-cell:** Hierarchical clustering has been widely used in genomics because it reveals relationships among clusters and subclusters. In gene expression studies, hierarchical clustering of genes or samples is often visualized as a heatmap with a dendrogram, allowing researchers to see nested clusters (e.g. gene families or cell subtypes). This method can detect small, rare clusters as leaves of the tree ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=in%20which%20each%20cluster%20is,part%20of%20the%20computational%20component)). It’s also intuitive: closely related samples will join on the tree before more divergent samples. In single-cell RNA-seq, many analysis pipelines in early studies used hierarchical clustering to identify cell types, especially when the cell count was modest (hundreds to a few thousand cells). For example, **SINCERA** (Guo et al. 2015) used hierarchical clustering with Pearson correlation distance to classify cell types ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=over,method%20to%20identify%20cell%20types)). The **BackSPIN** algorithm (Zeisel et al. 2015) performed divisive biclustering to simultaneously cluster neurons and genes in mouse brain data ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=CIDR%20%28Lin%20et%20al,is%20an%20agglomerative)). Hierarchical clustering is also used in population genetics (e.g. UPGMA or neighbor-joining trees built from genetic distance matrices) to illustrate how individuals or populations relate – effectively clustering individuals by genetic similarity.

**Use cases and tools:** In bulk transcriptomics (microarrays, RNA-seq), hierarchical clustering of genes was popularized by Eisen et al. (1998) as a way to find co-expressed gene modules – this remains common in genomic data analysis. In single-cell omics, hierarchical methods have often been combined with other techniques. For instance, **CIDR** (Lin et al. 2017) performs PCA on single-cell RNA-seq data and then applies agglomerative clustering to group cells ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=CIDR%20%28Lin%20et%20al,is%20an%20agglomerative)). The method **pcaReduce** (2016) uses iterative hierarchical merging of initial k-means clusters, yielding a hierarchy of cell groupings ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Several%20clustering%20tools%20based%20on,76)) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=genes%20and%20cells,method%20to%20identify%20cell%20types)). Hierarchical clustering is also useful for **pseudotime** or lineage analysis: after identifying clusters of cells, one might construct a hierarchical tree to hypothesize lineage relationships among cell types. In the GWAS realm, hierarchical clustering can group individuals by genotype similarity – for example, using identity-by-state distances, one can produce a dendrogram that mirrors population structure (though model-based approaches or PCA are more common in practice). Some population genetics tools perform hierarchical clustering on principal components or genetic distance; e.g., a study by Luca et al. 2012 (cited in  ([
            Clustering by genetic ancestry using genome-wide SNP data - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC3018397/#:~:text=We%20propose%20a%20different%20algorithm,does%20not%20require%20strict%20assumptions))) clustered subjects into genetically homogeneous groups to address stratification. More recently, pipelines like **PopMLvis** include hierarchical clustering as an option to cluster individuals after PCA ([PopMLvis: a tool for analysis and visualization of population structure using genotype data from genome-wide association studies | BMC Bioinformatics | Full Text](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-024-05908-1#:~:text=PopMLvis%20pipeline%2Fworkflow%3A%20,Download%20graphical%20plots%20and%20datasheets)).

**Scalability:** A drawback of hierarchical clustering is its computational cost. The naive agglomerative approach has time complexity on the order of *O(N^2)* or *O(N^3)* (depending on implementation and need to recompute distance matrix) and *O(N^2)* memory, since it often requires computing and storing an all-pairs distance matrix ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=The%20agglomerative%20hierarchical%20clustering%20has,number%20of%20clusters%2C%20it%20has)). This becomes infeasible for very large *N* (e.g. millions of single cells or hundreds of thousands of individuals). In practice, hierarchical clustering is limited to a few thousand points unless optimized methods (approximations or sparse representations) are used. Some single-cell methods avoid applying it to the full dataset; for example, they might cluster a smaller subset or cluster centroids of pre-clusters. Divisive methods can be even more computationally expensive (exponential in worst case) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=The%20agglomerative%20hierarchical%20clustering%20has,number%20of%20clusters%2C%20it%20has)). Because of these limitations, graph-based algorithms (next section) have largely replaced pure hierarchical clustering for large single-cell datasets ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Given%20the%20limitations%20of%20k,For)). 

- *Strengths:* Does not require choosing the number of clusters beforehand ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=strategies%20build%20a%20hierarchical%20structure,part%20of%20the%20computational%20component)). Produces a dendrogram that reveals a hierarchy of relationships – useful for understanding lineage or nested subpopulations. Can detect small clusters as distinct branches. No parametric assumptions about cluster shape or size (other than the chosen distance metric).  
- *Limitations:* Poor scalability to large datasets – quadratic or worse complexity makes it impractical for very large *N* ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=The%20agglomerative%20hierarchical%20clustering%20has,number%20of%20clusters%2C%20it%20has)). Merging or splitting decisions are irrevocable (greedy), which can lead to suboptimal partitions (e.g. early merge errors propagate). The choice of linkage and distance metric can greatly affect results (e.g. single linkage can cause chaining effects, complete linkage can break large clusters). There is no natural criterion to pick the “best” cut of the tree – determining the number of clusters still requires judgment or statistical metrics.  
- *Examples:* Clustering genes in a genome-wide expression matrix to find groups of co-regulated genes; clustering a manageable number of cells (hundreds) in early scRNA-seq studies to discover cell types; hierarchical clustering of cell type cluster centroids to suggest lineage relationships. Zeisel et al. (2015) used a hierarchical splitting algorithm to identify neuronal subtypes ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=CIDR%20%28Lin%20et%20al,is%20an%20agglomerative)). Lin et al. (2017) (CIDR) showed that hierarchical clustering on PCs could reliably group cells ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=CIDR%20%28Lin%20et%20al,is%20an%20agglomerative)). In population genomics, hierarchical clustering of individuals by genetic distance can recapitulate known population groupings (though model-based algorithms like STRUCTURE are more commonly used for that purpose).  
- *Software:* Many tools implement hierarchical clustering (e.g. hclust in R, SciPy or scikit-learn in Python). In single-cell workflows, it’s often used in downstream analysis (like clustering gene modules or building cluster trees) rather than for the main cell clustering step, due to scalability. Dedicated packages like **SC3** combine hierarchical clustering with other methods to improve stability ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=To%20overcome%20the%20above%20drawbacks%2C,suitable%20cluster%20number%2C%20which%20is)). By default, popular single-cell packages (Seurat, Scanpy, Monocle3) do not perform a full hierarchical clustering of all cells, but they may use hierarchical clustering internally for specific tasks (e.g. building cluster mergers or in plotting heatmaps of clusters).

## Louvain Clustering (Graph-Based Community Detection)  
**How it works:** Louvain clustering is a graph-based **community detection** algorithm originally developed to find communities in networks (Blondel et al. 2008). In the context of single-cell or genetic data, the items to cluster (cells or individuals) are represented as nodes in a graph, and edges connect similar nodes (for example, an edge between two cells if they are among each other’s *k* nearest neighbors in PCA space). The Louvain algorithm then iteratively optimizes a global objective called **modularity**, which measures the density of edges inside communities compared to edges between communities ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Louvain%20%28Blondel%20et%20al,the%20cell%20types%20for%20the)). It starts by assigning each node to its own cluster, then repeatedly merges clusters if the merge increases modularity. This yields a partition of the graph into communities (clusters). Louvain is a greedy algorithm but typically very fast: it runs in approximately O(N log N) time (where N is number of nodes) or even linear in the number of edges in practice ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=communities%20into%20a%20single%20node,the%20mapping%20of%20cellular%20localization)). A key feature is that the number of clusters is determined automatically by the algorithm’s modularity optimization; the user can tune a *resolution* parameter to get more or fewer clusters, but it’s not necessary to pre-set *K*. 

**Why it’s used in genetics/single-cell:** Graph-based clustering has proven especially powerful for single-cell data. After dimensionality reduction (e.g. PCA), one can construct a K-nearest-neighbor (KNN) graph of cells, which captures local similarity structure in the high-dimensional data ([10. Clustering — Single-cell best practices](https://www.sc-best-practices.org/cellular_structure/clustering.html#:~:text=The%20KNN%20graph%20consists%20of,61)). Dense regions of cells in gene expression space become clusters of highly connected nodes in the graph. Louvain clustering can then identify these communities efficiently. It is well-suited for single-cell transcriptomics because cell populations often do form dense subgraphs when projected into a suitable feature space, even if they are not well-separated by simple geometric distance. Unlike k-means, Louvain does not assume clusters are spherical – it can find arbitrarily shaped clusters as long as they manifest as network communities. It’s also robust to outliers (an outlier cell will simply have weak connections and often ends up isolated or in a tiny community). Critically, Louvain scales to very large numbers of cells: studies with 100k–1M cells have used it successfully ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=than%20other%20community,the%20mapping%20of%20cellular%20localization)). This scalability and ability to handle uneven cluster sizes led to graph-based methods becoming the **de facto standard** for scRNA-seq clustering in recent years ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Given%20the%20limitations%20of%20k,For)).

**Use cases and tools:** The introduction of graph clustering to single-cell analysis is often credited to the algorithm **PhenoGraph** (Levine et al. 2015), which applied Louvain community detection on a KNN graph of single-cell cytometry data, and was quickly adopted for scRNA-seq. The popular R package **Seurat** (Satija et al. 2015) also incorporated graph-based clustering early on, embedding cells in a shared nearest-neighbor graph and then finding communities (originally using the Louvain algorithm) ([Single-cell RNA-seq: Clustering Analysis | Introduction to Single-cell RNA-seq - ARCHIVED](https://hbctraining.github.io/scRNA-seq/lessons/07_SC_clustering_cells_SCT.html#:~:text=Seurat%20uses%20a%20graph,cliques%E2%80%99%20or%20%E2%80%98communities%E2%80%99)) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=than%20other%20community,the%20mapping%20of%20cellular%20localization)). This allowed Seurat to automatically find cell-type clusters without specifying the number of clusters *a priori*. Similarly, the Python toolkit **Scanpy** (Wolf et al. 2018) uses Louvain clustering (via its `tl.louvain` or `tl.leiden` functions) on KNN graphs, enabling analysis of over a million cells ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=communities%20into%20a%20single%20node,the%20mapping%20of%20cellular%20localization)). In these tools, the resolution parameter can be adjusted (higher resolution leads to more, smaller clusters; lower gives fewer, larger clusters). Louvain-based clustering has been used in countless single-cell studies — for example, to cluster peripheral blood mononuclear cells (PBMCs) into T cell, B cell, NK cell, etc., subsets, or to identify novel cell subtypes in complex tissues. In GWAS or population genetics, graph-based clustering is less commonly used, but one could construct a graph of individuals (edges connecting genetically similar individuals) and detect communities corresponding to populations. Indeed, “spectral graph theory” methods are mentioned for population structure detection ([
            Clustering by genetic ancestry using genome-wide SNP data - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC3018397/#:~:text=PCA%20,The)), and conceptually one could apply Louvain to a graph of individuals. However, PCA-based clustering or model-based approaches are more typical in that field, so Louvain’s biggest impact has been in single-cell biology.

**Scalability:** Louvain is very scalable and memory-efficient. The main cost is constructing the neighbor graph. For *n* cells and *k* neighbors per cell, the graph has ~ *n·k* edges. With *k* modest (e.g. 10–100), this is O(n) storage. Computing the *k*-NN graph can be done with efficient spatial indexing or approximate methods for large datasets. The Louvain optimization itself is fast: O(N log N) or better ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=communities%20into%20a%20single%20node,the%20mapping%20of%20cellular%20localization)), and can be further sped up by hierarchical graph coarsening. Louvain can easily handle tens or hundreds of thousands of nodes on a standard workstation, and with clustering frameworks, even millions of cells have been clustered ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=than%20other%20community,the%20mapping%20of%20cellular%20localization)). This makes it ideal for modern single-cell datasets. One caveat is that Louvain is a **greedy** algorithm and can have some randomness (e.g. the order of node moves can affect the outcome). It may get stuck in a suboptimal partition, and running it multiple times can yield slightly different clusterings. In practice, it’s stable for large well-separated communities, but small differences can occur for borderline assignments.

- *Strengths:* **No need to predefine cluster count.** It discovers however many clusters best fit the data’s community structure (controlled indirectly by a resolution parameter if desired). **Handles complex cluster shapes:** by operating on a neighbor graph, it clusters based on network connectivity, not purely distance – this can capture clusters that are not convex in the original space. **Highly scalable:** suitable for large single-cell datasets (hundreds of thousands of cells) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=than%20other%20community,the%20mapping%20of%20cellular%20localization)). **Widely validated:** has become a standard method yielding biologically meaningful cell clusters in scRNA-seq.  
- *Limitations:* Louvain’s optimization can occasionally merge small, distinct clusters if doing so only slightly decreases modularity (known as the “resolution limit” issue). It might overlook very rare cell populations if they are below the detectable community size unless one increases resolution. The algorithm can produce some randomness; small unstable clusters might differ between runs (though major clusters are usually consistent). These issues have largely been addressed by the **Leiden** algorithm (next section). Another limitation is that one must first choose a distance metric and construct the graph – if the wrong metric or too few neighbors are chosen, clustering can be affected. However, defaults (Euclidean on PCA space, ~10–30 neighbors) work well in many cases ([10. Clustering — Single-cell best practices](https://www.sc-best-practices.org/cellular_structure/clustering.html#:~:text=calculate%20a%20Euclidean%20distance%20matrix,61)).  
- *Examples:* Virtually all recent single-cell studies use Louvain or Leiden clustering. For instance, the Human Cell Atlas projects and numerous organ atlases cluster cells via graph community detection to define cell types. The Seurat tutorial on PBMCs shows graph-based clustering separating immune cell types ([Single-cell RNA-seq: Clustering Analysis | Introduction to Single-cell RNA-seq - ARCHIVED](https://hbctraining.github.io/scRNA-seq/lessons/07_SC_clustering_cells_SCT.html#:~:text=Seurat%20uses%20a%20graph,cliques%E2%80%99%20or%20%E2%80%98communities%E2%80%99)). Levine et al. (2015) used Louvain to cluster high-dimensional flow cytometry data, discovering new cell subsets. In one of the largest demonstrations, a 1.3 million cell RNA-seq dataset (10x Genomics) was clustered with a graph-based approach, underscoring the scalability.  
- *Software:* **Seurat** (R) uses a graph-based clustering by default (`FindClusters` function) – originally Louvain, with an option for the Leiden algorithm in newer versions. **Scanpy** (Python) provides `sc.tl.louvain` (now deprecated in favor of Leiden) and reports that their pipeline based on Louvain scales to >1e6 cells ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=communities%20into%20a%20single%20node,the%20mapping%20of%20cellular%20localization)). **Monocle 3** (R) for trajectory analysis also uses Louvain/Leiden to initially cluster cells into communities before learning trajectories ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Leiden%20%28Traag%20et%20al,it%20was%20proposed%20in%202020)) ([cluster_cells: Cluster cells using Louvain/Leiden community detection in cole-trapnell-lab/monocle3: Clustering, Differential Expression, and Trajectory Analysis for Single-Cell RNA-Seq](https://rdrr.io/github/cole-trapnell-lab/monocle3/man/cluster_cells.html#:~:text=might%20correspond%20to%20a%20different,function)). The algorithm is also implemented in many network analysis libraries. For very large data, one can use approximate kNN graph construction (e.g. annoy or faiss libraries in Scanpy) followed by Louvain/Leiden to speed up clustering.

## Leiden Clustering (Refined Graph-Based Method)  
**How it works:** Leiden is an improved graph community detection algorithm introduced by Traag et al. (2019) as an advancement over Louvain. It was designed to address known problems of Louvain: (1) the possibility of finding “improper” clusters (segments of clusters that are not fully connected, due to the greedy merge steps), (2) the resolution limit (difficulty in detecting very small clusters), and (3) potential instability in partitions ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Leiden%20%28Traag%20et%20al,it%20was%20proposed%20in%202020)). The Leiden algorithm modifies the procedure by adding a refinement phase and carefully ensuring that when nodes are moved between communities, the resulting clusters are **internally connected**. It also processes parts of the graph in a different order than Louvain, which leads to better partitions and faster convergence in practice ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Leiden%20%28Traag%20et%20al,it%20was%20proposed%20in%202020)). The outcome is that Leiden yields partitions of equal or higher modularity and guarantees that no cluster is disconnected (each community is a single connected component of the graph). Like Louvain, it doesn’t require a preset number of clusters – it optimizes modularity (or a related objective like CPM) and one can tune resolution. Leiden has similar or better time complexity; in practice it’s very fast and can be run on large datasets, sometimes even faster than Louvain due to quicker convergence.

**Why it’s used in genetics/single-cell:** Leiden rapidly gained popularity in the single-cell field as soon as it was released because it produces more consistent clustering results. Single-cell datasets often have *hierarchies* of clusters (e.g. big clusters that could be subdivided). Louvain might sometimes lump or split clusters suboptimally depending on initialization. Leiden, with its refinement step, tends to be more robust in detecting substructure without breaking apart connected groups. For example, if a tiny subpopulation of cells is only weakly connected to a larger cluster, Leiden is less likely to “over-merge” it into the large cluster (a problem Louvain could have without a high resolution setting). Conversely, Leiden also avoids one pitfall where Louvain could leave a few straggler cells in an otherwise separate cluster. Because of these advantages, Leiden has been recommended as the default for single-cell analysis by best-practice guidelines ([10. Clustering — Single-cell best practices](https://www.sc-best-practices.org/cellular_structure/clustering.html#:~:text=The%20Leiden%20algorithm%20is%20an,using%20Leiden%20instead%20is%20preferred)). It was quickly integrated into platforms like Seurat, Scanpy, and Monocle. In population genetics, Leiden is not specifically reported (since Louvain itself wasn’t standard there), but any scenario where graph clustering is applicable would benefit from Leiden’s improvements just as well.

**Use cases and tools:** By 2020, major single-cell toolkits adopted Leiden. **Scanpy** switched to recommending `sc.tl.leiden` for clustering, noting that Louvain is no longer maintained and Leiden produces superior results ([10. Clustering - Single-cell best practices](https://www.sc-best-practices.org/cellular_structure/clustering.html#:~:text=Since%20the%20Louvain%20algorithm%20is,Traag%20et%20al.%2C%202019%5D)) ([10. Clustering — Single-cell best practices](https://www.sc-best-practices.org/cellular_structure/clustering.html#:~:text=The%20Leiden%20algorithm%20is%20an,using%20Leiden%20instead%20is%20preferred)). **Seurat** added Leiden clustering (via the `FindClusters` function with an option to use the Leiden algorithm) – researchers using Seurat can simply specify the algorithm or use the default which may now be Leiden in recent versions. **Monocle 3** uses Leiden by default to cluster cells in its workflow ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Leiden%20%28Traag%20et%20al,it%20was%20proposed%20in%202020)) ([cluster_cells: Cluster cells using Louvain/Leiden community detection in cole-trapnell-lab/monocle3: Clustering, Differential Expression, and Trajectory Analysis for Single-Cell RNA-Seq](https://rdrr.io/github/cole-trapnell-lab/monocle3/man/cluster_cells.html#:~:text=cluster_cells%28%20cds%2C%20reduction_method%20%3D%20c%28,FALSE%2C%20resolution%20%3D%20NULL)). Essentially any pipeline that had Louvain now can use Leiden. Leiden has been used in studies to define very fine cell type granularity. For instance, if an scRNA-seq dataset has rare transitional cell states, Leiden is more likely to segregate those as distinct clusters at an appropriate resolution. Leiden was benchmarked in the original paper on multiple data types and showed better partition quality. It also can be applied iteratively to get multi-level clustering (similar to how Louvain can produce hierarchical community structure if one further clusters the condensed graph).

**Scalability:** Leiden is highly scalable, essentially the same order of complexity as Louvain. The refinement step adds a bit of overhead, but it’s not significant in big-O terms and often the algorithm converges in fewer overall iterations of the graph coarsening process because it improves partitions quickly. Traag et al. reported that Leiden is faster or at least not slower than Louvain on various benchmarks while giving better results ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=community%20division%3B%20,it%20was%20proposed%20in%202020)). Therefore, Leiden is suitable for large single-cell datasets (100k+ cells). Memory usage is similar to Louvain – dominated by storing the neighbor graph. In practice, if you can run Louvain on a dataset, you can run Leiden on it too. The stability of Leiden can also save time in analysis – one doesn’t need to run it multiple times to compare outcomes as sometimes done with Louvain.

- *Strengths:* Produces higher-quality clusters than Louvain, resolving small clusters and avoiding improperly merged groups ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Leiden%20%28Traag%20et%20al,it%20was%20proposed%20in%202020)). Ensures each cluster is contiguous in the graph (no accidental multi-component clusters). More **robust and deterministic** – results are less sensitive to random seed. Like Louvain, it does not require choosing cluster count and can handle arbitrary cluster shapes defined by the graph connectivity. Maintains the high speed and scalability. By now, it is considered best practice for single-cell clustering ([10. Clustering — Single-cell best practices](https://www.sc-best-practices.org/cellular_structure/clustering.html#:~:text=The%20Leiden%20algorithm%20is%20an,using%20Leiden%20instead%20is%20preferred)).  
- *Limitations:* Not many, relative to Louvain. One still must construct a meaningful graph (so all caveats about preprocessing, choosing the neighborhood size *k*, etc., apply equally to Leiden). Very tiny clusters (a few cells) can be missed if below resolution or if they don’t stand out by connectivity. Leiden also inherits the requirement of a resolution parameter if one wants to control cluster granularity explicitly (too high resolution could over-split clusters). But these are manageable. Essentially, Leiden’s limitations are just those of graph-based clustering in general, with fewer pitfalls than Louvain.  
- *Examples:* Virtually all new single-cell studies (2020 onward) that use Scanpy or Seurat are now using Leiden clustering to identify cell populations. For example, a complex bone marrow single-cell dataset might use Leiden to delineate subtle subpopulations of progenitor cells that Louvain might have merged ([10. Clustering — Single-cell best practices](https://www.sc-best-practices.org/cellular_structure/clustering.html#:~:text=The%20Leiden%20algorithm%20is%20an,using%20Leiden%20instead%20is%20preferred)). Best-practice tutorials show that Leiden outperforms other methods in various metrics for scRNA-seq clustering ([10. Clustering — Single-cell best practices](https://www.sc-best-practices.org/cellular_structure/clustering.html#:~:text=The%20Leiden%20algorithm%20is%20an,using%20Leiden%20instead%20is%20preferred)). In one comparison, Leiden was able to recover known cell types and rare subtypes more accurately than k-means or hierarchical clustering (Duò et al. 2018, Freytag et al. 2018) ([10. Clustering — Single-cell best practices](https://www.sc-best-practices.org/cellular_structure/clustering.html#:~:text=The%20Leiden%20algorithm%20is%20an,using%20Leiden%20instead%20is%20preferred)).  
- *Software:* As noted, **Scanpy** provides `tl.leiden` and by default suggests Leiden over Louvain. **Seurat** allows Leiden via the `algorithm = 4` option in FindClusters (or similar, depending on version) and many users have switched to that. **Monocle3** uses Leiden clustering in its `cluster_cells()` function by default ([cluster_cells: Cluster cells using Louvain/Leiden community detection in cole-trapnell-lab/monocle3: Clustering, Differential Expression, and Trajectory Analysis for Single-Cell RNA-Seq](https://rdrr.io/github/cole-trapnell-lab/monocle3/man/cluster_cells.html#:~:text=reduction_method%20%3D%20c%28,FALSE%2C%20nn_control%20%3D%20list)). Other graph clustering libraries (igraph, etc.) have Leiden implementations as well. The algorithm is generic, so any analysis of genetic networks or sample-sample networks could in principle use Leiden for community detection.

## DBSCAN (Density-Based Clustering)  
**How it works:** DBSCAN (Density-Based Spatial Clustering of Applications with Noise; Ester et al. 1996) is a density-based algorithm that groups together points that are packed closely in space and labels points in sparse regions as outliers (noise) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Density,its%20clustering%20method%20is%20based)). Two main parameters govern DBSCAN: **ε (epsilon)** – the neighborhood radius, and **minPts** – the minimum number of points required to form a dense region. The algorithm considers a point a “core point” if at least *minPts* points (including itself) fall within distance ε of it. Core points that are within ε of each other are part of the same cluster. Points within ε of a core point are called border points and are assigned to that core point’s cluster (even if they themselves don’t have *minPts* neighbors). Points that are not within ε of any core point are labeled as noise ([DBSCAN Clustering in ML | Density based clustering | GeeksforGeeks](https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/#:~:text=DBSCAN%20works%20by%20categorizing%20data,points%20into%20three%20types)) ([DBSCAN Clustering in ML | Density based clustering | GeeksforGeeks](https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/#:~:text=1,This%20chaining%20process)). In essence, DBSCAN will find clusters as contiguous regions of high point density, and it can find clusters of arbitrary shape. It does **not require specifying the number of clusters** upfront – clusters emerge from the data based on density. The algorithm is typically O(N log N) with index-assisted neighbor queries, making it relatively efficient for large N ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=most%20popular%20density,a%20design%20may%20lead%20to)), though performance can degrade in very high dimensions.

**Why it’s suitable for genetics/single-cell:** DBSCAN is attractive for data where clusters are irregularly shaped or when one expects a lot of background noise points. In single-cell genomics, one scenario is identifying rare cell types: these might appear as small dense pockets of cells in gene expression space, separated by low-density regions from the main clusters. DBSCAN can detect such small clusters by using a small ε, and mark the rest as noise or part of larger clusters ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=parameters%20including%20%C9%9B%20,differentially%20expressed%20genes%20between%20cells)). This approach was taken by **GiniClust** (Jiang et al. 2016), which focused on detecting rare cell populations – it selected genes with high Gini index (to highlight rare-cell-specific expression) and applied DBSCAN to find tight micro-clusters of rare cells ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=parameters%20including%20%C9%9B%20,differentially%20expressed%20genes%20between%20cells)). Because DBSCAN doesn’t force every point into a cluster, it can identify outlier cells (which might be doublets, low-quality cells, or truly novel cell types) as noise. Another use is clustering of spatial or genetic data where cluster shapes are non-convex. For example, if cells form a continuum or a chain in expression space, k-means might split them incorrectly or hierarchical might chain everything into one cluster, whereas DBSCAN could separate distinct dense regions. In population genetics, one could imagine using DBSCAN to find clusters of individuals in a PCA plot where clusters are not nicely spherical (though in practice this is not commonly done; population structure tends to be simpler or handled by model-based methods). DBSCAN (and its variants) are also applicable to identifying clusters of SNPs in genomic coordinates (like in physical mapping, but that’s more a genomic interval clustering which is a different context).

 ([File:DBSCAN-density-data.svg - Wikipedia](https://en.wikipedia.org/wiki/File:DBSCAN-density-data.svg)) *Example of arbitrarily shaped clusters that DBSCAN can identify.* Here DBSCAN finds a compact cluster (blue) and a crescent-shaped cluster (green) even though one encloses the other, and it marks outliers as noise (gray). Unlike centroid-based methods, DBSCAN can discover clusters of any shape and even detect a cluster completely surrounded by a different cluster ([DBSCAN - Wikipedia](https://en.wikipedia.org/wiki/DBSCAN#:~:text=1,sitting%20on%20the%20edge%20of)). This ability is useful when genetic or transcriptomic data contain complex manifolds or nested subpopulations.

Once an appropriate ε is chosen, DBSCAN can naturally handle clusters of differing sizes and shapes. It’s worth noting that after dimensionality reduction (e.g. projecting cells into UMAP or t-SNE space), researchers sometimes use density-based clustering on that 2D/3D embedding. For instance, one might run HDBSCAN (a hierarchical variant of DBSCAN) on a UMAP plot of cells to cluster them without having to choose *K*. This can sometimes pick up fine structure that graph clustering might merge, or confirm that visually distinct UMAP groups correspond to DBSCAN clusters.

**Use cases and tools:** Aside from GiniClust in single-cell transcriptomics ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=parameters%20including%20%C9%9B%20,differentially%20expressed%20genes%20between%20cells)), another notable use of density clustering is in **Monocle 2** (Qiu et al. 2017). Monocle 2 introduced a “density peak clustering” algorithm (Rodriguez & Laio 2014) to initially cluster cells in order to order them along trajectories. This method is different from DBSCAN but is in the same spirit of density-based clustering – it finds cluster centers as density peaks. Monocle’s density-peak approach allowed identification of cell states without predefining cluster number, analogous to DBSCAN’s ability to find clusters by density alone. In the realm of genomics, DBSCAN has been used on somatic mutation patterns or spatial transcriptomics coordinates, where clusters might be irregularly shaped in some latent space. Another use is clustering of genetic variants by linkage disequilibrium patterns – though not standard, one could imagine density-based clustering in a high-dimensional LD space to find groups of correlated variants. Generally, DBSCAN and related algorithms are less common in mainstream single-cell workflows compared to graph-based methods, but they remain a valuable approach in niche applications, especially for *detecting outliers or rare events*. Tools like **Scanpy** include DBSCAN (via scikit-learn) as an alternative clustering option (`sc.tl.cluster` can call DBSCAN if desired). The **HDBSCAN** library is also used by some for single-cell embedding clustering due to its ability to handle varying densities and automatically select clusters.

**Scalability:** DBSCAN can handle large datasets, but a challenge is the distance computations in high dimensions. With indexing (e.g. k-d trees or ball trees), DBSCAN achieves O(N log N) for low-medium dimensions. For gene expression with, say, 50 PCA components, this is manageable for tens of thousands of cells. However, if one attempted DBSCAN on 20,000-dimensional raw gene vectors, it would be both slow and likely ineffective (distance in such high-D space becomes noisy). So, dimensionality reduction is usually done first. Memory-wise, one does not need to store all pairwise distances, only to be able to query neighborhoods. Libraries like **scikit-learn** implement DBSCAN efficiently. The *minPts* parameter is often set to something like D+1 (where D is dimensionality) by rule of thumb ([DBSCAN Clustering in ML | Density based clustering | GeeksforGeeks](https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/#:~:text=,to%20form%20a%20dense%20region)) ([DBSCAN Clustering in ML | Density based clustering | GeeksforGeeks](https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/#:~:text=DBSCAN%20works%20by%20categorizing%20data,points%20into%20three%20types)). Choosing ε is critical and can require examining a k-distance graph to find the distance at which neighborhoods “jump” in size. DBSCAN’s performance also depends on data distribution; if the data has very uneven density, a single ε might not suit all clusters (HDBSCAN can help in that case by varying the scale).

- *Strengths:* **No pre-set number of clusters needed.** Clusters emerge from dense regions, and outliers are automatically identified as noise ([DBSCAN - Wikipedia](https://en.wikipedia.org/wiki/DBSCAN#:~:text=1,sitting%20on%20the%20edge%20of)) ([DBSCAN - Wikipedia](https://en.wikipedia.org/wiki/DBSCAN#:~:text=3,expert%2C%20if%20the%20data%20is)). Can find clusters of **arbitrary shape**, even one cluster surrounding another ([DBSCAN - Wikipedia](https://en.wikipedia.org/wiki/DBSCAN#:~:text=1,sitting%20on%20the%20edge%20of)) – something k-means and hierarchical (with Euclidean distance) struggle with. Naturally handles **noise/outliers**, which is useful for single-cell data containing doublets or low-quality cells. DBSCAN is conceptually simple and only needs intuitive parameters (neighborhood size and minimum cluster size).  
- *Limitations:* The results can be sensitive to the choice of ε and minPts – these parameters need tuning and domain knowledge ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=most%20popular%20density,a%20design%20may%20lead%20to)) ([DBSCAN - Wikipedia](https://en.wikipedia.org/wiki/DBSCAN#:~:text=2,be%20chosen%20appropriately%20for%20all)). If clusters have very different densities, a single ε may cause DBSCAN to merge some clusters or fragment others ([DBSCAN - Wikipedia](https://en.wikipedia.org/wiki/DBSCAN#:~:text=2,be%20chosen%20appropriately%20for%20all)). In high dimensions, determining a meaningful ε is difficult due to the curse of dimensionality ([DBSCAN - Wikipedia](https://en.wikipedia.org/wiki/DBSCAN#:~:text=2,be%20chosen%20appropriately%20for%20all)) – often distances to nearest neighbors become very similar. DBSCAN also tends to break large gradual gradients into one cluster (which might be fine or not depending on context). It’s not deterministic if border points can be reached from two clusters (though this is a minor effect) ([DBSCAN - Wikipedia](https://en.wikipedia.org/wiki/DBSCAN#:~:text=1,distance%20measure%20used%20in%20the)). Another limitation is that DBSCAN doesn’t provide a hierarchy or multiple resolutions (though HDBSCAN addresses that by creating a hierarchy of clusters by varying ε). For very large datasets (e.g. >1e5 points), DBSCAN might be slower than graph-based methods unless using approximations, because constructing an approximate kNN graph and running Louvain/Leiden might be faster than range searching for every point.  
- *Examples:* GiniClust used DBSCAN to successfully identify rare cell types (clusters containing just a handful of cells) that were missed by other methods ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=parameters%20including%20%C9%9B%20,differentially%20expressed%20genes%20between%20cells)). In a tumor single-cell dataset, DBSCAN could be used to isolate a small cluster of tumor cells with a unique mutation profile as “outliers” apart from larger clusters of normal cells. In flow cytometry, density-based clustering is essentially what manual gating does – DBSCAN automates a similar idea in multi-dimensional marker space, identifying dense immunophenotype populations and treating sparse regions as background. Another example is clustering single-cell ATAC-seq peaks by signal patterns across cells; one could imagine density-based clustering picking out groups of peaks with similar chromatin accessibility patterns across many samples.  
- *Software:* DBSCAN is implemented in **scikit-learn** (`DBSCAN` class) and thus accessible in Python workflows (Scanpy can use it). R has DBSCAN in packages like `dbscan`. **HDBSCAN** (Python package) is often used on UMAP embeddings of single-cell data by the community, as it can automatically choose the number of clusters. However, major single-cell pipelines by default use graph clustering rather than DBSCAN. Specialized methods like **GiniClust** encapsulate DBSCAN for the user, automatically setting parameters to find rare clusters ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=parameters%20including%20%C9%9B%20,differentially%20expressed%20genes%20between%20cells)). Monocle’s density peak clustering is available in Monocle 2’s R package (`clusterCells` with method = “densityPeak”). In summary, DBSCAN is available but usually optional in single-cell analysis; it’s most useful to supplement graph clustering when one suspects very unusual cluster shapes or wants to identify outliers.

## Other Clustering Approaches and Considerations  

Beyond the main algorithms above, several other clustering techniques are used in genetics and single-cell research, each with their own contexts:

### Spectral Clustering  
Spectral clustering involves computing eigenvectors of a similarity (or graph Laplacian) matrix of the data and using these to partition points. In practice, one constructs an affinity matrix (e.g. Gaussian similarity between gene expression profiles) or a kNN graph, computes the top *k* eigenvectors, and then clusters points in that reduced spectral space (often with k-means). Spectral clustering can capture complex cluster structures because it essentially performs a graph partitioning. It has been applied to single-cell data: for example, **SIMLR** (Wang et al. 2017) is a method that uses multiple kernel similarities and spectral clustering to group single cells ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=similarity%20matrix,impose%20a%20nonnegative%20and%20low)). SIMLR was shown to handle large noisy scRNA-seq datasets and can integrate multiple data types ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=similarity%20matrix,to%20miss%20patterns%20in%20data)). Another method, **SinNLRR** (Zheng et al. 2019), builds a low-rank similarity matrix and then applies spectral clustering ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=SIMLR%20has%20an%20advantage%20in,requires%20users%20to%20set%20the)). The advantage of spectral clustering is its flexibility – it doesn’t presume convex clusters and can separate complex intertwined patterns (much as graph-based methods do). However, classical spectral clustering requires computing eigen-decomposition of an N×N matrix, which is O(N^3) and memory-heavy for large N ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Spectral%20clustering%20is%20a%20widely,data%20sets%20with%20heavy%20noise)). This limits its direct use for very large single-cell datasets. Approaches like SIMLR circumvent this by clever optimizations or by operating on a subset of data. In genetics, spectral techniques appear in population structure inference (e.g. spectral graph theory is mentioned for detecting substructure ([
            Clustering by genetic ancestry using genome-wide SNP data - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC3018397/#:~:text=PCA%20,The)), and PCA itself is essentially spectral decomposition of a covariance matrix). But spectral *clustering* per se (discrete grouping via eigenvectors) is less often explicitly used in GWAS studies. It’s more likely to be seen in integrative genomics (clustering samples by multi-omic similarity networks, etc.). In summary, spectral clustering is powerful but computationally intensive; it often underpins more specialized algorithms in single-cell analysis rather than being used on its own by end-users.

- *Pros:* Can handle very complex cluster shapes by capturing the data’s manifold structure in eigenvectors. Does not force a specific cluster geometry in original space. The number of clusters can be inferred (e.g. by eigen-gap or using k-means in spectral space).  
- *Cons:* Poor scalability for large N (eigen-decomposition is expensive) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Spectral%20clustering%20is%20a%20widely,data%20sets%20with%20heavy%20noise)). Requires setting number of clusters (or using heuristics to guess it). Needs careful construction of similarity graph (choice of kernel/affinity). For single-cell data with tens of thousands of cells, naive spectral clustering is impractical without approximations.  
- *Use case:* SIMLR applied spectral clustering to a few thousand cells and showed it could detect cell types across datasets with high noise ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=reduction%2C%20clustering%2C%20and%20visualization%20of,requires%20users%20to%20set%20the)). Spectral clustering might also be used on smaller datasets or as a baseline method in clustering benchmarks. It’s conceptually related to the graph-based methods; in fact, the first step of constructing a kNN graph is common, but Louvain/Leiden use a different strategy than eigen-decomposition to get clusters.  
- *Software:* There are implementations in sklearn (though not scalable to large N). SIMLR has an R/Bioconductor package. One could also manually do spectral clustering by computing a Laplacian in R (e.g. `igraph` or `kernlab` packages) or Python (numpy/scipy for eigen-decomposition), but again data size is a limiting factor. 

### Model-Based Clustering (Gaussian Mixture Models and Others)  
Model-based clustering assumes data are generated from a mixture of probabilistic distributions. The most common is a Gaussian Mixture Model (GMM), where each cluster is modeled as a Gaussian distribution in the feature space. The Expectation-Maximization (EM) algorithm is typically used to fit a GMM, yielding both cluster assignments and probabilities (soft clustering). GMMs can be more flexible than k-means because one can allow clusters to have different sizes and shapes (through covariance matrices), rather than just equal-radius spheres. In genomics, finite mixture models have been used for clustering when a statistical framework is desired. For example, clustering gene expression profiles via a mixture of Gaussians or t-distributions can capture clusters with different expression variance. However, in single-cell RNA-seq, GMMs haven’t been as popular due to the high dimensionality and noise (and the difficulty of estimating full covariance matrices for high-D Gaussians). There are some recent efforts combining deep learning and GMMs (e.g. **scVAE** by Grönbech et al. 2020 used a variational autoencoder with a mixture model for clustering ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=%28Chen%20et%20al,seq%20and%20bulk%20data)), and **scGMAI** (Yang et al. 2021) uses an autoencoder + GMM for scRNA-seq). These approaches effectively reduce dimensionality with neural networks then apply a GMM in latent space. In population genetics, a famous model-based approach is the **STRUCTURE** algorithm (Pritchard et al. 2000), which clusters individuals into populations by modeling allele frequency distributions – it’s not Gaussian, but a discrete mixture model (each individual as a mixture of populations). STRUCTURE and its derivatives (ADMIXTURE) are widely used to infer population clusters (K ancestral populations) from genotype data, and they require specifying K but provide probabilities of assignment to each cluster for each individual (accounting for admixture). While not Gaussian, they illustrate the model-based clustering philosophy in genetics. Another example: in cancer genomics, mixture models are used to cluster subclonal populations based on variant allele frequencies.

- *Pros:* Provides a statistical framework – one can compute likelihoods, use information criteria to choose number of clusters, and get uncertainty estimates (e.g. an individual belongs to cluster A with 90% probability, cluster B with 10%). GMM can model clusters of different shapes if full covariance is used, or enforce spherical clusters by restricting covariance (which then mimics k-means). Model-based methods can be extended with prior knowledge (e.g. Dirichlet process mixtures allow an infinite mixture model that can grow clusters as needed).  
- *Cons:* EM for GMM can get stuck in local optima and may require multiple restarts. It’s computationally costly for large data, especially if full covariance matrices are used (parameters grow with dimension squared). High-dimensional gene expression data can violate Gaussian assumptions (zero-inflation, nonlinear relationships). Also, one usually needs to specify the number of mixture components or use heavy methods to infer it (like Bayesian model selection). For single-cell data with thousands of cells, a GMM with full covariance could be intractable; diagonal covariance (assuming uncorrelated features) or working in a reduced PCA space is more feasible but still might not outperform simpler methods.  
- *Use case:* GMM clustering might be applied to a low-dimensional embedding of single-cell data. For instance, cluster centers in a UMAP plot could be refined by fitting Gaussians – but since UMAP axes have arbitrary units, this is heuristic. In bulk gene expression, one might cluster samples into subtypes using a GMM on principal components, allowing clusters to differ in variance structure. In population genetics, model-based clustering like STRUCTURE is very common to identify genetic populations, though these are custom models rather than generic GMMs.  
- *Software:* R’s **mclust** package performs GMM clustering with options for covariance structures and can even suggest number of clusters via BIC. Scikit-learn has a `GaussianMixture` for Python. STRUCTURE and ADMIXTURE are specialized tools for genotype data (not Gaussian, but worth mentioning as a model-based clustering approach in genetics). Newer single-cell methods like **scVI** (Lopez et al. 2018) learn a latent space of gene expression – while scVI itself doesn’t explicitly cluster, one could feed its latent variables into a GMM for clustering; indeed some pipelines use scVI for integration then k-means or GMM for clustering cell types.

### Consensus and Ensemble Clustering  
Given the variability of clustering results with different algorithms or parameters, ensemble approaches have been developed. **SC3** (Single Cell Consensus Clustering) is a prime example: it runs multiple clustering algorithms on the single-cell data (k-means with different Ks, hierarchical with different parameters, etc.), then creates a consensus matrix and derives a final clustering that is stable across methods ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=To%20overcome%20the%20above%20drawbacks%2C,suitable%20cluster%20number%2C%20which%20is)). This can yield more robust clusters that don’t depend on one algorithm’s quirks. SC3 was shown to outperform single methods in detecting cell types on smaller datasets (up to a few thousand cells) but is computationally intensive (since it runs many clusterings). Another ensemble approach is to run a method like k-means multiple times with different initializations (or use different algorithms altogether) and then take an agreement (for example, majority voting for each pair of cells being in the same cluster). In genetics, consensus clustering has been used in some cancer subtyping studies to assess the stability of clusters of patients across many clustering runs (e.g. Monti et al. 2003 introduced consensus clustering in class discovery). This is less about a specific algorithm and more about combining algorithms.

- *Pros:* Improved robustness and confidence in clusters. Can capture complementary strengths of algorithms (e.g. k-means might split obvious large clusters while hierarchical finds fine substructure – consensus could integrate that). Often provides a stability score for clusters.  
- *Cons:* Computationally heavy, not feasible for very large datasets without significant resources. The resolution/detail of clusters might be limited by the need for consensus (could merge what one method would split). Interpretation can be complex.  
- *Use case:* SC3 on single-cell RNA-seq for up to ~5000 cells provides the user with high-confidence clusters and even an interactive visualization. Ensemble clustering in population genetics isn’t common, but one could imagine combining PCA-based clustering and model-based clustering results to see a consensus population structure.  
- *Software:* **SC3** (R Bioconductor) is specifically for single-cell and integrates consensus clustering. General frameworks for consensus clustering exist in R (`ConsensusClusterPlus`) and Python.

### Deep Learning–Based Clustering  
Recently, methods that incorporate neural networks have emerged. These typically use an autoencoder or other deep model to learn a lower-dimensional representation or denoised features of the data, and then perform clustering in that latent space (sometimes simultaneously optimizing the clustering objective and the representation). For example, **DESC** (Li et al. 2020) uses a deep autoencoder to embed scRNA-seq data and iteratively refines clusters (with a soft clustering assignment) to also correct batch effects ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Recently%2C%20a%20few%20deep%20learning%C2%AD%E2%80%93based,means%20algorithm%20to%20cluster)). **scDeepCluster** (Tian et al. 2019) and **scziDesk** (Chen et al. 2020) similarly combine autoencoders with clustering, using techniques like clustering loss terms or self-training with k-means in the loop ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=for%20the%20batch%20correction%20since,seq%20and%20bulk%20data)). **scVAE** (Grønbech et al. 2020) employs a variational autoencoder that directly models data as mixture of Gaussians in latent space, achieving jointly a reduced representation and clustering ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=%28Chen%20et%20al,seq%20and%20bulk%20data)). The appeal of deep learning methods is that they can model complex nonlinear relationships in the data (going beyond PCA) and potentially account for technical noise (dropouts, batch effects) during clustering. They can also scale to fairly large datasets by stochastic optimization, rather than computing large similarity matrices.

- *Pros:* Can yield more meaningful feature representations for clustering by denoising data. Potentially handles technical variation (like batch correction integrated with clustering). Often, these methods don’t require manual feature selection since the autoencoder learns what is important. Some, like variational approaches, give uncertainty estimates. They typically still don’t require choosing cluster number (though some do, or they treat it as hyperparameter to tune).  
- *Cons:* Neural network models require more expertise to train (hyperparameters, risk of overfitting if data is small, etc.). They can be like “black boxes,” making it harder to interpret why cells are clustered a certain way. Training can be time-consuming (though still often faster than trying to do spectral clustering on huge data, for example). They also may require substantial computational resources (GPUs for large datasets to train efficiently). Moreover, improvements over classical methods are not guaranteed; for many clustering tasks, simpler methods suffice unless the data has specific challenges.  
- *Use case:* Large single-cell atlases with complex batch effects – DESC was demonstrated to group cells by cell type rather than batch by jointly clustering and correcting batch differences ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Recently%2C%20a%20few%20deep%20learning%C2%AD%E2%80%93based,means%20algorithm%20to%20cluster)). Another use is when dealing with multimodal data: deep models can integrate multiple data types (e.g. RNA and ATAC from the same cell) and cluster in a joint latent space. In genetics, deep learning for clustering is less common, but one could envision autoencoders for genotype data to find population structure, or for single-cell multi-omics.  
- *Software:* **DESC** and **scDeepCluster** provide Python implementations (TensorFlow/PyTorch-based). **scVI** (while aimed at integration) can be seen as a deep generative model from which clustering can be derived. There’s also **DIMM-SC** and others in literature. These are active research areas, not yet as standardized as k-means or Louvain in everyday analysis.

### Final Notes on Usage  
In practice, single-cell researchers often use a combination of methods: for example, graph-based clustering (Louvain/Leiden) to get initial groups, followed by hierarchical clustering of those group centroids to visualize relationships, and perhaps sub-clustering certain groups with a different algorithm if needed. In genetics, one might use PCA (spectral) to reduce dimensions, then k-means or hierarchical to cluster individuals, and validate those clusters with model-based approaches. It is also common to try multiple algorithms to see if results agree. Each algorithm’s performance can depend on data characteristics (cluster size, separation, noise level, dimensionality). Comparative studies (Duò et al. 2018; Freytag et al. 2018; Weber & Robinson 2016) have found that for single-cell RNA-seq, graph-based methods (Louvain/Leiden) tend to outperform simpler methods in recovering known cell types ([10. Clustering — Single-cell best practices](https://www.sc-best-practices.org/cellular_structure/clustering.html#:~:text=The%20Leiden%20algorithm%20is%20an,using%20Leiden%20instead%20is%20preferred)), which explains their ubiquity in recent years. Still, understanding all these methods is useful because specialized situations (like finding a rare cell type, or clustering a very small dataset, or dealing with continuous trajectories) might call for a different approach.

**Summary:**  
- **K-means** – fast and simple partitioning; widely used historically in gene expression clustering; needs *K* defined, assumes spherical clusters; scales well but can miss small or oddly-shaped clusters ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=However%2C%20k,detecting%20of%20rare%20cell%20types)).  
- **Hierarchical clustering** – builds a cluster tree; good for revealing nested structure and rare clusters without choosing *K* ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=in%20which%20each%20cluster%20is,part%20of%20the%20computational%20component)); not scalable to huge data (use for smaller sets or as part of hybrid methods); often used in gene clustering and visualizations.  
- **Louvain (graph clustering)** – dominant method for large single-cell data; constructs a kNN graph and finds communities by modularity optimization ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Louvain%20%28Blondel%20et%20al,the%20cell%20types%20for%20the)); very scalable and handles complex cluster shapes; no need for preset cluster count; forms basis of Seurat/Scanpy pipelines.  
- **Leiden (graph clustering)** – improved community detection fixing Louvain’s issues; now the recommended default in single-cell analysis due to more accurate and stable results ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Leiden%20%28Traag%20et%20al,it%20was%20proposed%20in%202020)) ([10. Clustering — Single-cell best practices](https://www.sc-best-practices.org/cellular_structure/clustering.html#:~:text=The%20Leiden%20algorithm%20is%20an,using%20Leiden%20instead%20is%20preferred)); equally scalable; implemented in Seurat, Scanpy, Monocle, etc.  
- **DBSCAN (density clustering)** – finds arbitrarily shaped clusters based on point density, identifies noise/outliers ([DBSCAN Clustering in ML | Density based clustering | GeeksforGeeks](https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/#:~:text=Unlike%20K,world%20data%20irregularities%20such%20as)) ([DBSCAN - Wikipedia](https://en.wikipedia.org/wiki/DBSCAN#:~:text=1,sitting%20on%20the%20edge%20of)); no preset *K* needed; useful for detecting small clusters or outliers in single-cell data (e.g. rare cell types) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=parameters%20including%20%C9%9B%20,differentially%20expressed%20genes%20between%20cells)); requires tuning ε; less used than graph methods but valuable in niche scenarios.  
- **Spectral clustering** – uses eigenvalues of similarity matrix; adaptive to complex structures; has been used via methods like SIMLR ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=similarity%20matrix,to%20miss%20patterns%20in%20data)); limited by computational cost for large datasets.  
- **Model-based clustering (GMM/Structure)** – probabilistic approach; common in population genetics (STRUCTURE for ancestry) and some gene expression analyses; provides soft assignments; computationally heavy for high dimensions; not mainstream in single-cell due to noise and dimensionality.  
- **Deep learning methods** – new approaches combining autoencoders and clustering (DESC, scVI, etc.); address noise and batch effects; promising results but added complexity; not yet standard in most pipelines, but likely to grow in use for very large or complex multi-dimensional datasets.

By understanding these algorithms’ mechanics and trade-offs, researchers can choose the method best suited to their data – or use multiple methods to cross-validate that the clusters (cell types, patient subgroups, or genetic populations) they discover are robust and biologically meaningful. Each method contributes a piece to the clustering toolkit in genetics and single-cell research, enabling us to interpret ever-larger and higher-dimensional genomic datasets. 

**References:** The content above integrates information from key publications and resources: the review by Zhang et al. (2023) which outlines clustering methods in single-cell RNA-seq ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=performed,and%20compare%20those%20clustering%20methods)) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Louvain%20%28Blondel%20et%20al,the%20mapping%20of%20cellular%20localization)) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Leiden%20%28Traag%20et%20al,it%20was%20proposed%20in%202020)) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Density,types%20since%20it%20can%20be)), the documentation of single-cell analysis best practices ([10. Clustering — Single-cell best practices](https://www.sc-best-practices.org/cellular_structure/clustering.html#:~:text=The%20Leiden%20algorithm%20is%20an,using%20Leiden%20instead%20is%20preferred)), and foundational algorithm descriptions (e.g. Lloyd 1982 for k-means ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=k,Lloyd%201982)), Ester et al. 1996 for DBSCAN ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=Density,its%20clustering%20method%20is%20based)), Blondel et al. 2008 for Louvain, Traag et al. 2019 for Leiden). We also cited specific examples like Grün et al. 2015 (RaceID) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=2016%20,76)), Kiselev et al. 2017 (SC3) ([
            Review of single-cell RNA-seq data clustering for cell-type identification and characterization - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10158997/#:~:text=To%20overcome%20the%20above%20drawbacks%2C,suitable%20cluster%20number%2C%20which%20is)), and others to illustrate use cases in genetics and single-cell contexts.

